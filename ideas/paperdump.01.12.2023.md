LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes
https://arxiv.org/pdf/2311.13384.pdf

Cross-Image Attention for Zero-Shot Appearance Transfer
https://arxiv.org/pdf/2311.03335.pdf

GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions
https://arxiv.org/pdf/2311.16037.pdf

Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing
https://arxiv.org/pdf/2311.16043.pdf

Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling
https://arxiv.org/pdf/2311.16096.pdf

SODA: Bottleneck Diffusion Models for Representation Learning
https://arxiv.org/pdf/2311.17901.pdf

Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions
https://qtransformer.github.io/assets/qtransformer.pdf

On Bringing Robots Home
https://arxiv.org/pdf/2311.16098.pdf

Adversarial Diffusion Distillation
https://static1.squarespace.com/static/6213c340453c3f502425776e/t/65663480a92fba51d0e1023f/1701197769659/adversarial_diffusion_distillation.pdf

MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers
https://nihalsid.github.io/mesh-gpt/static/MeshGPT.pdf

SiRA: Sparse Mixture of Low Rank Adaptation
https://arxiv.org/pdf/2311.09179.pdf

LONGQLORA: EFFICIENT AND EFFECTIVE METHOD TO EXTEND CONTEXT LENGTH OF LARGE LANGUAGE MODELS
https://arxiv.org/pdf/2311.04879v2.pdf

JARV IS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models
https://arxiv.org/pdf/2311.05997.pdf

LONGLORA: EFFICIENT FINE-TUNING OF LONGCONTEXT LARGE LANGUAGE MODELS
https://arxiv.org/pdf/2309.12307.pdf

Video-LLaVA: Learning United Visual Representation by Alignment Before Projection
https://arxiv.org/pdf/2311.10122.pdf

Adaptive Shells for Efficient Neural Radiance Field Rendering
https://arxiv.org/pdf/2311.10091.pdf

Exponentially Faster Language Modeling
https://arxiv.org/pdf/2311.10770.pdf

Break the Sequential Dependency of LLM Inference Using Lookahead Decoding
https://lmsys.org/blog/2023-11-21-lookahead-decoding/

Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey
https://arxiv.org/pdf/2311.12351.pdf

Let’s Verify Step by Step
https://arxiv.org/pdf/2305.20050.pdf

PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF
https://arxiv.org/pdf/2311.13099.pdf

Ring Attention with Blockwise Transformers for Near-Infinite Context
https://arxiv.org/pdf/2310.01889.pdf

Scalable AI Safety via Doubly-Efficient Debate
https://arxiv.org/pdf/2311.14125.pdf

Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts
https://arxiv.org/pdf/2306.04723.pdf

The Internal State of an LLM Knows When It’s Lying
https://arxiv.org/pdf/2304.13734.pdf

Sequence Length is a Domain: Length-based Overfitting in Transformer Models
https://arxiv.org/pdf/2109.07276.pdf

Meet in the Middle: A New Pre-training Paradigm
https://arxiv.org/pdf/2303.07295.pdf